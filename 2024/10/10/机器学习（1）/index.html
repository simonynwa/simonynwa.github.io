<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>机器学习笔记（1） | simonynwa's blog</title><meta name="author" content="Simon Ding"><meta name="copyright" content="Simon Ding"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="一. 特征工程1. 为什么要对特征做归一化处理Feature scaling，特征归一化，标准化。特征间的单位（尺度）可能不同，尺度大的特征会起决定性作用。为了消除单位和尺度的影响，以对每维特征同等看待，需要对其进行归一化。 2. 什么是组合特征？如何处理高维组合特征？为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。可以采取降维，矩阵分解或者特征筛选的方法得">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习笔记（1）">
<meta property="og:url" content="http://example.com/2024/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89/index.html">
<meta property="og:site_name" content="simonynwa&#39;s blog">
<meta property="og:description" content="一. 特征工程1. 为什么要对特征做归一化处理Feature scaling，特征归一化，标准化。特征间的单位（尺度）可能不同，尺度大的特征会起决定性作用。为了消除单位和尺度的影响，以对每维特征同等看待，需要对其进行归一化。 2. 什么是组合特征？如何处理高维组合特征？为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。可以采取降维，矩阵分解或者特征筛选的方法得">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/cover.jpg">
<meta property="article:published_time" content="2024-10-10T15:10:23.000Z">
<meta property="article:modified_time" content="2024-10-10T15:37:43.095Z">
<meta property="article:author" content="Simon Ding">
<meta property="article:tag" content="笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/cover.jpg"><link rel="shortcut icon" href="/img/web.jpg"><link rel="canonical" href="http://example.com/2024/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习笔记（1）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-10-10 23:37:43'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="simonynwa's blog" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/avartar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">3</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/cover.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="simonynwa's blog"><span class="site-name">simonynwa's blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">机器学习笔记（1）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-10-10T15:10:23.000Z" title="发表于 2024-10-10 23:10:23">2024-10-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-10-10T15:37:43.095Z" title="更新于 2024-10-10 23:37:43">2024-10-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="机器学习笔记（1）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h3 id="一-特征工程"><a href="#一-特征工程" class="headerlink" title="一. 特征工程"></a>一. 特征工程</h3><h4 id="1-为什么要对特征做归一化处理"><a href="#1-为什么要对特征做归一化处理" class="headerlink" title="1. 为什么要对特征做归一化处理"></a>1. 为什么要对特征做归一化处理</h4><p>Feature scaling，特征归一化，标准化。特征间的单位（尺度）可能不同，尺度大的特征会起决定性作用。为了消除单位和尺度的影响，以对每维特征同等看待，需要对其进行归一化。</p>
<h4 id="2-什么是组合特征？如何处理高维组合特征？"><a href="#2-什么是组合特征？如何处理高维组合特征？" class="headerlink" title="2. 什么是组合特征？如何处理高维组合特征？"></a>2. 什么是组合特征？如何处理高维组合特征？</h4><p>为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组合，构成高阶组合特征。可以采取降维，矩阵分解或者特征筛选的方法得到具备绝大部分信息的几个特征进行训练。</p>
<h4 id="3-比较欧式距离和曼哈顿距离、"><a href="#3-比较欧式距离和曼哈顿距离、" class="headerlink" title="3. 比较欧式距离和曼哈顿距离、"></a>3. 比较欧式距离和曼哈顿距离、</h4><p>欧式距离：欧几里得距离，就是平方和开根号。欧式距离越小，两个向量的相似度越大。<br>曼哈顿距离：$|x1-x2|+|y1-y2|$<br>切比雪夫距离：$max(|x1-x2|, |y1-y2|)$</p>
<h4 id="4-什么是余弦相似度？为什么有些场景下使用余弦相似度？"><a href="#4-什么是余弦相似度？为什么有些场景下使用余弦相似度？" class="headerlink" title="4. 什么是余弦相似度？为什么有些场景下使用余弦相似度？"></a>4. 什么是余弦相似度？为什么有些场景下使用余弦相似度？</h4><p>两个向量夹角的余弦，关注的是向量之间的角度关系，并不关心它们的绝对大小,余弦相似度依然符合“相同为1，正交为0，相反为-1”的性质。<br>欧式距离衡量空间点的直线距离，余弦距离衡量点在空间的方向差异。欧式距离体现的数值上的绝对差异，而余弦距离体现方向上的相对差异。</p>
<h4 id="5-one-hot的作用是什么？为什么不能直接用数字来表示？"><a href="#5-one-hot的作用是什么？为什么不能直接用数字来表示？" class="headerlink" title="5. one-hot的作用是什么？为什么不能直接用数字来表示？"></a>5. one-hot的作用是什么？为什么不能直接用数字来表示？</h4><p>One-Hot编码是分类变量作为二进制向量的表示。这首先要求将分类值映射到整数值。然后，每个整数值被表示为二进制向量，除了整数的索引之外，它都是零值，它被标记为1。直接使用数字会给将人工误差而导致的假设引入到类别特征中，比如类别之间的大小关系，以及差异关系等等，且直接用数字不是连续的。</p>
<h3 id="二-模型评估"><a href="#二-模型评估" class="headerlink" title="二. 模型评估"></a>二. 模型评估</h3><h4 id="1-过拟合和欠拟合是啥？具体表现是啥？"><a href="#1-过拟合和欠拟合是啥？具体表现是啥？" class="headerlink" title="1. 过拟合和欠拟合是啥？具体表现是啥？"></a>1. 过拟合和欠拟合是啥？具体表现是啥？</h4><p>  过拟合：训练集上表现很好，测试集上表现很差。<br>  欠拟合：训练集上表现很差。欠拟合会导致高bias，过拟合会导致高variance，所以模型需要在bias和variance之间做出一个权衡。</p>
<h4 id="2-降低过拟合和欠拟合的方法"><a href="#2-降低过拟合和欠拟合的方法" class="headerlink" title="2. 降低过拟合和欠拟合的方法"></a>2. 降低过拟合和欠拟合的方法</h4><ol>
<li>解决欠拟合的方法：</li>
</ol>
<ul>
<li>增加新特征，可以考虑加入特征组合，高次特征，来增大假设空间。</li>
<li>尝试非线性模型，比如核SVM，决策树，DNN</li>
<li>如果有正则项可以调小正则项参数$\lambda$</li>
<li>Boosting, 比如Gradient Boosting、</li>
</ul>
<ol start="2">
<li>解决过拟合的方法：</li>
</ol>
<ul>
<li>减少特征数量</li>
<li>正则化</li>
</ul>
<h4 id="3-L1和L2正则分别服从什么分布"><a href="#3-L1和L2正则分别服从什么分布" class="headerlink" title="3. L1和L2正则分别服从什么分布"></a>3. L1和L2正则分别服从什么分布</h4><p>L1是拉普拉斯分布，L2是高斯分布。范数是一个函数，是矢量空间内的所有矢量赋予非零的正<strong>长度</strong>或<strong>大小</strong>。<br>$$||x||<em>p &#x3D; (\sum^n</em>{i&#x3D;1}|x_i|^p)^{1&#x2F;p}$$<br>L1范数：当p&#x3D;1时，是L1范数，其表示某个向量中所有元素**绝对值的和。</p>
<p>L2范数：当p&#x3D;2时，是L2范数， 表示某个向量中所有元素平方和再开根， 也就是欧几里得距离公式。</p>
<h4 id="4-对于树形结构为什么不用归一化？"><a href="#4-对于树形结构为什么不用归一化？" class="headerlink" title="4. 对于树形结构为什么不用归一化？"></a>4. 对于树形结构为什么不用归一化？</h4><p>因为数值缩放不影响分裂点位置，对树模型的结构不造成影响。按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。而且，树模型是不能进行梯度下降的，<strong>因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，并且求导没意义，也就不需要归一化。</strong> 树型结构不关心变量的值，只关心变量分布以及变量之间的条件概率。</p>
<h4 id="5-什么是数据不平衡？如何解决？"><a href="#5-什么是数据不平衡？如何解决？" class="headerlink" title="5. 什么是数据不平衡？如何解决？"></a>5. 什么是数据不平衡？如何解决？</h4><p>数据不平衡主要存在于有监督机器学习任务中。当遇到数据不平衡时，以总体分类准确率为学习目标的传统分类算法会过多地关注多数类，从而使得少数样本的分类性能下降。绝大多数常见的机器学习算法对于不平衡数据集都不能很好地工作。</p>
<h4 id="6-bias，variance，error"><a href="#6-bias，variance，error" class="headerlink" title="6. bias，variance，error"></a>6. bias，variance，error</h4><p>Error &#x3D; Bias + Variance + Noise，偏差（bias）指的是模型的预测值与真实值之间的系统性误差。方差（variance）指的是模型对训练数据的敏感程度，也就是模型在不同训练数据集上的表现差异。噪声（noise）指的是数据中不可避免的随机误差，它通常来自测量误差、数据不完整或数据中的内在随机性。噪声是数据固有的一部分，与模型无关。</p>
<h3 id="三-线性回归与逻辑回归"><a href="#三-线性回归与逻辑回归" class="headerlink" title="三. 线性回归与逻辑回归"></a>三. 线性回归与逻辑回归</h3><h4 id="1-logistic回归公式是什么？"><a href="#1-logistic回归公式是什么？" class="headerlink" title="1. logistic回归公式是什么？"></a>1. logistic回归公式是什么？</h4><p>逻辑回归虽然名字里面有回归，但是主要用来解决分类问题。logistic-regression就是一个线性回归经过阶跃函数的处理，变成一个二项分类器，输出结果只能是0，1的条件概率的大小，其实是一种概率模型。<br>$$\large<br>P(Y&#x3D;1|x) &#x3D; \frac{1}{1+e^{-(w^Tx+b)}}<br>$$</p>
<p>也就是说，输出 Y&#x3D;1 的对数几率是由输入 x 的<strong>线性函数</strong>表示的模型，这就是<strong>逻辑回归模型</strong>。当 WT+b 的值越接近正无穷， P(Y&#x3D;1|x) 概率值也就越接近 1。因此<strong>逻辑回归的思路</strong>是，先拟合决策边界(不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。</p>
<h4 id="2-逻辑回归和线性回归，有何异同？"><a href="#2-逻辑回归和线性回归，有何异同？" class="headerlink" title="2. 逻辑回归和线性回归，有何异同？"></a>2. 逻辑回归和线性回归，有何异同？</h4><p>逻辑回归与线性回归都属于广义线性回归模型, 逻辑回归处理的是分类问题，线性回归处理的是回归问题，这是两者最本质的区别。线性回归中使用的是最小化平方误差损失函数（线性回归就是y&#x3D;WT+b，损失函数用MSE）</p>
<h4 id="3-逻辑回归的损失函数"><a href="#3-逻辑回归的损失函数" class="headerlink" title="3. 逻辑回归的损失函数"></a>3. 逻辑回归的损失函数</h4><p>极大似然估计法: 就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！那么既然事情已经发生了，为什么不让这个出现的结果的可能性最大呢？这也就是最大似然估计的核心。</p>
<p>白球黑球不知道有多少，取100次有放回，70是白，30黑，那么采取的方法是让这个样本结果出现的可能性最大，也就是使得p^70(1-p)^30值最大，那么我们就可以看成是p的方程，求导即可！50，50。20，80等无数种组合都可以，为什么要70和30呢？因为是已经发生的事情了，就让他作为可能性最大的结果。</p>
<h4 id="4-逻辑回归处理多分类标签问题时，一般怎么做？"><a href="#4-逻辑回归处理多分类标签问题时，一般怎么做？" class="headerlink" title="4. 逻辑回归处理多分类标签问题时，一般怎么做？"></a>4. 逻辑回归处理多分类标签问题时，一般怎么做？</h4><p>如果一个样本只对应于一个标签，我们可以假设每个样本属于不同标签的概率服从于几何分布，使用多项逻辑回归（ Softmax Regression ) 来进行分类。当存在样本可能居于多个标签的情况时，我们可以训练 k个二分类的逻辑回归分类器。第l个分类器用以区分每个样本是否可以归为第i类，训练该分类器时，需要把标签重新整理为“第 i 类标签” 与“非第i类标签”两类。 遇过这样的办法，我们就解决了每个样本可能拥有多个标签的情况。</p>
<p>多项逻辑回归:<br>    $$\large P(Y&#x3D;k|x) &#x3D; \frac{e^{w_k^Tx + b_k}}{\sum_{j&#x3D;1}^Ke^{w_j^Tx+b_j}}$$</p>
<h3 id="四-朴素贝叶斯模型"><a href="#四-朴素贝叶斯模型" class="headerlink" title="四. 朴素贝叶斯模型"></a>四. 朴素贝叶斯模型</h3><h4 id="1-全概率公式-贝叶斯公式"><a href="#1-全概率公式-贝叶斯公式" class="headerlink" title="1. 全概率公式 &amp; 贝叶斯公式"></a>1. 全概率公式 &amp; 贝叶斯公式</h4><p>$$\large P(A|B) &#x3D; \frac{P(B|A)P(A)}{\sum_{i&#x3D;1}^nP(B|A_i)P(A_i)}$$</p>
<p>贝叶斯：$$P(A|B) &#x3D; \frac{P(B|A)P(A)}{P(B)}$$<br>全概率：<br>$$P(A) &#x3D; \sum_{i&#x3D;1}^nP(A|B_i)$$</p>
<h4 id="2-为什么叫朴素贝叶斯？"><a href="#2-为什么叫朴素贝叶斯？" class="headerlink" title="2. 为什么叫朴素贝叶斯？"></a>2. 为什么叫朴素贝叶斯？</h4><p>因为<strong>分母对于所有类别为常数</strong>，我们只要将分子最大化即可。又因为各特征属性是条件独立的(朴素贝叶斯为什么“朴素”的原因)。</p>
<h4 id="3-工作流程？"><a href="#3-工作流程？" class="headerlink" title="3. 工作流程？"></a>3. 工作流程？</h4><p>对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。通俗来说，就好比这么个道理，你在街上看到一个黑人，我问你你猜这哥们哪里来的，你十有八九猜非洲。为什么呢？因为黑人中非洲人的比率最高，当然人家也可能是美洲人或亚洲人，但在没有其它可用信息下，我们会选择条件概率最大的类别，这就是朴素贝叶斯的思想基础。</p>
<h4 id="4-优缺点"><a href="#4-优缺点" class="headerlink" title="4. 优缺点"></a>4. 优缺点</h4><p>优点：</p>
<ol>
<li><p>易于实现和理解：朴素贝叶斯算法基于简单的概率论和统计学知识，易于实现和理解。</p>
</li>
<li><p>高效性：朴素贝叶斯算法的计算成本相对较低，对于大规模数据集的训练和分类具有很高的效率。</p>
</li>
<li><p>对缺失数据不敏感：在朴素贝叶斯算法中，某个特征值的缺失不会影响整个实例的分类。</p>
</li>
<li><p>稳健性：在数据分布符合模型假设的情况下，朴素贝叶斯分类器通常表现出较好的分类性能。</p>
</li>
<li><p>适用于多分类问题：朴素贝叶斯算法可以处理多分类问题，并且可以增量式学习。</p>
</li>
</ol>
<p>缺点：</p>
<ol>
<li><p>独立性假设：朴素贝叶斯算法假设特征之间是相互独立的，这在现实中往往不成立。这种假设可能导致算法在特征相关性较高的情况下表现不佳。</p>
</li>
<li><p>对输入数据的表达形式敏感：朴素贝叶斯算法的性能受到输入数据表达形式的影响。如果数据的预处理或特征选择不合适，可能会影响分类效果。</p>
</li>
<li><p>对参数估计敏感：朴素贝叶斯算法的性能也依赖于参数估计的准确性。如果样本数据不足以准确估计参数，分类器的性能可能会受到影响。</p>
</li>
</ol>
<h3 id="六-集成学习"><a href="#六-集成学习" class="headerlink" title="六. 集成学习"></a>六. 集成学习</h3><h4 id="1-什么是集成学习算法？"><a href="#1-什么是集成学习算法？" class="headerlink" title="1. 什么是集成学习算法？"></a>1. 什么是集成学习算法？</h4><p>集成学习算法本身不算一种单独的机器学习算法，而是通过构建并结合多个机器学习来完成学习任务。可以说是集百家之所长，能在机器学习算法中拥有较高的准确率，不足之处就是模型的训练过程可能比较复杂，效率不是很高。</p>
<p><strong>目前常见的集成学习算法主要有2种：基于Bagging的算法和基于Boosting的算法，</strong> 基于Bagging的代表算法有随机森林，而基于Boosting的代表算法则有Adaboost、GBDT、XGBOOST等。</p>
<h4 id="2-集成学习主要有哪些框架？"><a href="#2-集成学习主要有哪些框架？" class="headerlink" title="2. 集成学习主要有哪些框架？"></a>2. 集成学习主要有哪些框架？</h4><p>Bagging（并行），Boosting（串行）， Stacking</p>
<h4 id="3-常用的bagging（Boostrap-Agregation）算法有哪些？"><a href="#3-常用的bagging（Boostrap-Agregation）算法有哪些？" class="headerlink" title="3. 常用的bagging（Boostrap Agregation）算法有哪些？"></a>3. 常用的bagging（Boostrap Agregation）算法有哪些？</h4><p>多次采样（bootstrap （有放回的随机抽样）每次从训练集中抽取一个固定大小的训练集A,随机重抽样），训练多个分类器，集体投票，旨在减小方差，防止过拟合。</p>
<p>如果是<strong>分类</strong>算法预测，则T个弱学习器<strong>投出最多票数</strong>的类别或者类别之一为最终类别。如果是<strong>回归</strong>算法，T个弱学习器得到的回归结果进行<strong>算术平均</strong>得到的值为最终的模型输出。常用bagging算法：<strong>随机森林</strong>算法。</p>
<h4 id="4-常用的Boosting算法有哪些？"><a href="#4-常用的Boosting算法有哪些？" class="headerlink" title="4. 常用的Boosting算法有哪些？"></a>4. 常用的Boosting算法有哪些？</h4><p>基分类器层层叠加，聚焦分错的样本，旨在减小方差。</p>
<p>Boosting的主要思想：<strong>迭代式学习。</strong>  </p>
<p>AdaBoost，GBDT，XGBoost都属于Boosting思想。</p>
<p><strong>涉及到两个部分，加法模型和前向分步算法。</strong><br>加法模型就是说强分类器由一系列弱分类器线性相加而成。<br>前向分步就是说在训练过程中，下一轮迭代产生的分类器是在上一轮的基础上训练得来的。</p>
<p>Boosting是一种与Bagging很类似的技术。它的基本原理：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，<strong>提高被错误分类的样本的权重</strong>，<strong>降低被正确分类的样本的权重</strong>，<strong>使得先前基学习器做错的训练样本在后续受到更多的关注</strong>，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最后将这T个基学习器进行加权结合。</p>
<p>目标：减少模型的bias，提高预测准确性<br>流程：1、给定初始训练数据，由此训练出第一个基学习器；2、根据基学习器的表现对样本进行调整，在之前学习器做错的样本上投入更多关注；3、用调整后的样本，训练下一个基学习器；4、重复上述过程T次，将T个学习器加权结合</p>
<h4 id="5-常用的stacking算法有哪些？"><a href="#5-常用的stacking算法有哪些？" class="headerlink" title="5. 常用的stacking算法有哪些？"></a>5. 常用的stacking算法有哪些？</h4><p><strong>多次采样，训练多个分类器，将输出作为最后的输入特征,</strong> 由k-NN、随机森林和朴素贝叶斯基础分类器组成，它的预测结果由作为元分类器的Logistic 回归组合。</p>
<p>基模型 —&gt; 新的特征集 —&gt; 元模型（使用基模型的预测结果作为输入，并学习如何最佳地组合这些预测结果来给出最终的预测）—&gt; 最终预测</p>
<h3 id="七-随机森林"><a href="#七-随机森林" class="headerlink" title="七.  随机森林"></a>七.  随机森林</h3><h4 id="1-简述随机森林算法原理"><a href="#1-简述随机森林算法原理" class="headerlink" title="1. 简述随机森林算法原理"></a>1. 简述随机森林算法原理</h4><p>随机森林是一种以决策树（<strong>分类树或者回归树都可以</strong>）为基分类器的集成算法，通过组合多棵独立的决策树后根据<strong>投票或取均值</strong>的方式得到最终预测结果的机器学习方法。</p>
<ul>
<li>一个样本容量为N的样本，有放回的抽取N次，每次抽取1个，最终形成了N个样本。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。</li>
<li>当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m &lt;&lt; M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。</li>
<li>决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。</li>
<li>按照步骤1~3建立大量的决策树，这样就构成了随机森林了。</li>
<li>多数投票机制进行预测。（<strong>分类用投票，回归用均值</strong>）</li>
</ul>
<h4 id="2-随机森林的随机性体现在哪？"><a href="#2-随机森林的随机性体现在哪？" class="headerlink" title="2. 随机森林的随机性体现在哪？"></a>2. 随机森林的随机性体现在哪？</h4><p>随机森林的随机性体现在每颗树的<strong>训练样本是随机的</strong>，树中每个节点的分裂属性集合也是随机选择确定的。有了这2个随机的保证，随机森林就不会产生过拟合的现象了。</p>
<h4 id="3-随机森林算法的优缺点"><a href="#3-随机森林算法的优缺点" class="headerlink" title="3. 随机森林算法的优缺点"></a>3. 随机森林算法的优缺点</h4><p>优点：它能够处理很高维度的数据，并且不用做特征选择，因为特征子集是随机选择的；训练时树与树之间是相互独立的，训练速度快，容易做成并行化方法；对缺失值不敏感，如果有很大一部分的特征遗失，仍可以维持准确度。</p>
<p>缺点：随机森林在某些噪音较大的分类或回归问题上会过拟合；对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响</p>
<h4 id="4-随机森林为什么不能用全样本去训练m棵决策树？"><a href="#4-随机森林为什么不能用全样本去训练m棵决策树？" class="headerlink" title="4. 随机森林为什么不能用全样本去训练m棵决策树？"></a>4. 随机森林为什么不能用全样本去训练m棵决策树？</h4><p>全样本训练忽视了局部样本的规律，对于模型的泛化能力是有害的（如果有m个决策树，那就需要m个一定数量的样本集来训练每一棵树）</p>
<h4 id="5-随机森林和GBDT的区别"><a href="#5-随机森林和GBDT的区别" class="headerlink" title="5. 随机森林和GBDT的区别"></a>5. 随机森林和GBDT的区别</h4><p>组成<strong>随机森林</strong>的树可以是分类树，也可以是回归树；而<strong>GBDT</strong>只能由回归树组成。 组成<strong>随机森林</strong>的树可以并行生成；而<strong>GBDT</strong>只能是串行生成。 对于最终的输出结果而言，<strong>随机森林</strong>采用多数投票等；而<strong>GBDT</strong>则是将所有结果累加起来，或者加权累加起来。</p>
<h3 id="八-Adaboost（自适应增强）"><a href="#八-Adaboost（自适应增强）" class="headerlink" title="八. Adaboost（自适应增强）"></a>八. Adaboost（自适应增强）</h3><h4 id="1-简述Adaboost原理"><a href="#1-简述Adaboost原理" class="headerlink" title="1. 简述Adaboost原理"></a>1. 简述Adaboost原理</h4><p>它的自适应在于：前一个基本分类器被错误分类的样本的权值会增大，而正确分类的样本的权值会减小，并再次用来训练下一个基本分类器。同时，在每一轮迭代中，加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数才确定最终的强分类器。</p>
<h4 id="2-Adaboost常用的损失函数有哪些？"><a href="#2-Adaboost常用的损失函数有哪些？" class="headerlink" title="2. Adaboost常用的损失函数有哪些？"></a>2. Adaboost常用的损失函数有哪些？</h4><p>指数损失</p>
<p>对于二分类问题，假设有样本 $(x_i, y_i)$，其中 $x_i$​ 是输入特征，$y_i$​ 是标签，通常取值为$y_i \in {-1, +1}$$。假设模型的输出是 f(x_i)$，那么指数损失定义为：<br>$$\large L(y_i, f(x_i)) &#x3D; e^{-y_if(x_i)}$$<br>其中：</p>
<ul>
<li>如果样本 $(x_i, y_i)$ 被正确分类，即 $y_i f(x_i) &gt; 0$，损失较小；</li>
<li>如果样本 $(x_i, y_i)$  被错误分类，即 $y_i f(x_i) &lt; 0$，损失会非常大。</li>
</ul>
<h4 id="3-Adaboost如何用于分类？"><a href="#3-Adaboost如何用于分类？" class="headerlink" title="3. Adaboost如何用于分类？"></a>3. Adaboost如何用于分类？</h4><p>步骤：</p>
<ul>
<li>计算样本权重：训练数据中的每个样本，赋予其权重，即样本权重，用向量D表示，这些权重都初始化成相等值。假设有n个样本的训练集，设定每个样本的权重都是相等的，即1&#x2F;n。</li>
<li>计算错误率：利用第一个弱学习算法h1对其进行学习，学习完成后进行错误率的统计</li>
<li>计算弱学习算法权重：弱学习算法也有一个权重，用向量α表示，利用错误率计算权重α：  α&gt;0，错误率&lt;0.5, 错误率越小，α越大。错误率&#x3D;0.5，α&#x3D;1；α&lt;0:错误率很大</li>
<li>更新样本权重：在第一次学习完成后，需要重新调整样本的权重，以使得在第一分类中被错分的样本的权重，在接下来的学习中可以重点对其进行学习；其中，h（xi）&#x3D;yi表示对第i个样本训练正确，不等于则表示分类错误。Z是一个归一化因子：Zt&#x3D;sum(D)</li>
<li>重复进行学习：这样经过t轮的学习后，就会得到t个弱学习算法、权重、弱分类器的输出以及最终的AdaBoost算法的输出：H(X)&gt;0 是1类；H(X)&lt;0 是-1类</li>
</ul>
<h4 id="4-Adaboost算法的优缺点？"><a href="#4-Adaboost算法的优缺点？" class="headerlink" title="4. Adaboost算法的优缺点？"></a>4. Adaboost算法的优缺点？</h4><p>优点：无需对弱分类器进行过度优化 不容易过拟合<br>缺点：对噪声敏感，需要强分类能力的弱分类器 对类别不平衡数据敏感</p>
<h3 id="九-GBDT"><a href="#九-GBDT" class="headerlink" title="九. GBDT"></a>九. GBDT</h3><h4 id="1-简述GBDT原理"><a href="#1-简述GBDT原理" class="headerlink" title="1. 简述GBDT原理"></a>1. 简述GBDT原理</h4><p>DT - Decision Tree决策树，GB是Gradient Boosting，是一种学习策略，GBDT的含义就是用Gradient Boosting的策略训练出来的DT模型。</p>
<p>Gradient Boost Decision Tree。总共构建T棵树，。当构建到第t棵树的时候，需要对前t-1棵树的训练样本分类回归产生的残差进行拟合。每次构建树的方式以及数据集一样，只不过拟合的目标变成了t-1棵树输出的残差。也就是说，这棵新树要学习如何去“纠正”之前t-1棵树的错误。不可并行化处理。GBDT是一种boosting算法。</p>
<p><strong>GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差</strong>，这个<strong>残差就是一个加预测值后能得真实值的累加量</strong>。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。</p>
<h4 id="2-GBDT常用的损失函数有哪些？"><a href="#2-GBDT常用的损失函数有哪些？" class="headerlink" title="2. GBDT常用的损失函数有哪些？"></a>2. GBDT常用的损失函数有哪些？</h4><p>回归任务：</p>
<ol>
<li><p>MAE (mean absolute error) 对异常值不敏感<br>$$\large L(y,\hat{y}) &#x3D; \frac{1}{n}\sum_{i&#x3D;1}^{n}|y_i-\hat{y}_i|$$</p>
</li>
<li><p>MSE（mean squared error) 对大误差敏感<br>$$\large L(y,\hat{y}) &#x3D; \frac{1}{n}\sum_{i&#x3D;1}^{n}(y_i-\hat{y}_i)^2$$</p>
</li>
<li><p>RMSE（root mean squared error）<br>$$\large L(y,\hat{y}) &#x3D; \sqrt{\frac{1}{n}\sum_{i&#x3D;1}^{n}(y_i-\hat{y}_i)^2}$$</p>
</li>
</ol>
<p>分类任务：</p>
<ol>
<li><p>对数损失（logarithmic loss）：<br>$$\large L(y,\hat{y}) &#x3D; -\frac{1}{n}\sum_{i&#x3D;1}^{n}[y_ilog(\hat{y}_i)+(1-y_i)log(1-\hat{y}_i)]$$<br>对数损失（也称为二元交叉熵损失）用于二分类问题。</p>
</li>
<li><p>指数损失（exponential loss）：<br>$$\large L(y,\hat{y}) &#x3D; exp(-y \cdot \hat{y})$$<br>指数损失是AdaBoost算法使用的损失函数，它倾向于更强烈地惩罚错误分类，导致模型更加注重难分类的样本。</p>
</li>
</ol>
<h4 id="3-GBDT如何用于分类"><a href="#3-GBDT如何用于分类" class="headerlink" title="3. GBDT如何用于分类"></a>3. GBDT如何用于分类</h4><p>与回归任务类似，不同的是损失函数不同。</p>
<h4 id="4-GBDT算法的优缺点？"><a href="#4-GBDT算法的优缺点？" class="headerlink" title="4. GBDT算法的优缺点？"></a>4. GBDT算法的优缺点？</h4><p>优点：</p>
<ol>
<li>高精度</li>
<li>灵活：可应用于多种类型的数据</li>
<li>无需特征归一化，对特征的尺度不敏感</li>
</ol>
<p>缺点：</p>
<ol>
<li>无法并行，计算时间长</li>
<li>难以处理高维稀疏数据</li>
</ol>
<h3 id="十-XGBoost"><a href="#十-XGBoost" class="headerlink" title="十.  XGBoost"></a>十.  XGBoost</h3><h4 id="1-简述XGBoost"><a href="#1-简述XGBoost" class="headerlink" title="1. 简述XGBoost"></a>1. 简述XGBoost</h4><p>总共构建T颗树。当构建到第t颗树的时候，需要对前t-1颗树对训练样本分类回归产生的残差进行拟合。每次拟合产生新的树的时候，遍历所有可能的树，并选择使得目标函数值（cost）最小的树。但是这样在实践中难以实现，因此需要将步骤进行分解，在构造新的树的时候，每次只产生一个分支，并选择最好的那个分支。如果产生分支的目标函数值（cost）比不产生的时候大或者改进效果不明显，那么就放弃产生分支（相当于truncate，截断）。可以并行化处理，效率比GBDT高，效果比GBDT好。XGBoost是一种boosting算法。</p>
<h4 id="2-XGBoost和GBDT有什么不同？"><a href="#2-XGBoost和GBDT有什么不同？" class="headerlink" title="2. XGBoost和GBDT有什么不同？"></a>2. XGBoost和GBDT有什么不同？</h4><ol>
<li>在XGBoost中，特征分裂的计算可以并行进行</li>
<li>传统的GBDT通常没有在目标函数中显式加入正则化项，仅通过调节参数（如树的深度、学习率、树的数量等）来控制模型的复杂度。XGBoost在目标函数中引入了正则化项（L1和L2正则化），直接对模型的复杂度进行惩罚。</li>
<li>传统的GBDT算法通常只使用损失函数的一阶导数（梯度）来指导模型的更新。XGBoost不仅利用了一阶导数（梯度），还使用了损失函数的二阶导数（Hessian矩阵）。</li>
<li></li>
</ol>
<ul>
<li><strong>GBDT</strong>：GBDT在每次构建树时，按照增益选择最佳分裂点，构建完整的树结构，并在需要时进行预剪枝（Pre-pruning），即限制树的深度或叶子节点数量以控制模型复杂度。</li>
<li><strong>XGBoost</strong>：XGBoost首先构建一棵完整的树，然后从叶子节点开始向上进行后剪枝（Post-pruning）。这意味着在构建树的过程中，XGBoost能够根据增益来决定是否保留某个分裂节点，进一步减少过拟合的风险。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Simon Ding</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2024/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89/">http://example.com/2024/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">simonynwa's blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a></div><div class="post_share"><div class="social-share" data-image="/cover.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%882%EF%BC%89/" title="机器学习笔记（2）"><img class="cover" src="/cover.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">机器学习笔记（2）</div></div></a></div><div class="next-post pull-right"><a href="/2024/10/10/python%E8%80%83%E7%82%B9/" title="python考点"><img class="cover" src="/cover.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">python考点</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/10/10/python%E8%80%83%E7%82%B9/" title="python考点"><img class="cover" src="/cover.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-10</div><div class="title">python考点</div></div></a></div><div><a href="/2024/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%882%EF%BC%89/" title="机器学习笔记（2）"><img class="cover" src="/cover.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-10</div><div class="title">机器学习笔记（2）</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/avartar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Simon Ding</div><div class="author-info__description">我的个人博客</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">3</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">1</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/simonynwa"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/simonynwa" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:dingyuang@sjtu.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-number">1.</span> <span class="toc-text">一. 特征工程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%AF%B9%E7%89%B9%E5%BE%81%E5%81%9A%E5%BD%92%E4%B8%80%E5%8C%96%E5%A4%84%E7%90%86"><span class="toc-number">1.1.</span> <span class="toc-text">1. 为什么要对特征做归一化处理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%BB%80%E4%B9%88%E6%98%AF%E7%BB%84%E5%90%88%E7%89%B9%E5%BE%81%EF%BC%9F%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E9%AB%98%E7%BB%B4%E7%BB%84%E5%90%88%E7%89%B9%E5%BE%81%EF%BC%9F"><span class="toc-number">1.2.</span> <span class="toc-text">2. 什么是组合特征？如何处理高维组合特征？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%AF%94%E8%BE%83%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB%E5%92%8C%E6%9B%BC%E5%93%88%E9%A1%BF%E8%B7%9D%E7%A6%BB%E3%80%81"><span class="toc-number">1.3.</span> <span class="toc-text">3. 比较欧式距离和曼哈顿距离、</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E4%BB%80%E4%B9%88%E6%98%AF%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E4%BA%9B%E5%9C%BA%E6%99%AF%E4%B8%8B%E4%BD%BF%E7%94%A8%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6%EF%BC%9F"><span class="toc-number">1.4.</span> <span class="toc-text">4. 什么是余弦相似度？为什么有些场景下使用余弦相似度？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-one-hot%E7%9A%84%E4%BD%9C%E7%94%A8%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E8%83%BD%E7%9B%B4%E6%8E%A5%E7%94%A8%E6%95%B0%E5%AD%97%E6%9D%A5%E8%A1%A8%E7%A4%BA%EF%BC%9F"><span class="toc-number">1.5.</span> <span class="toc-text">5. one-hot的作用是什么？为什么不能直接用数字来表示？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="toc-number">2.</span> <span class="toc-text">二. 模型评估</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88%E6%98%AF%E5%95%A5%EF%BC%9F%E5%85%B7%E4%BD%93%E8%A1%A8%E7%8E%B0%E6%98%AF%E5%95%A5%EF%BC%9F"><span class="toc-number">2.1.</span> <span class="toc-text">1. 过拟合和欠拟合是啥？具体表现是啥？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E9%99%8D%E4%BD%8E%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-number">2.2.</span> <span class="toc-text">2. 降低过拟合和欠拟合的方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-L1%E5%92%8CL2%E6%AD%A3%E5%88%99%E5%88%86%E5%88%AB%E6%9C%8D%E4%BB%8E%E4%BB%80%E4%B9%88%E5%88%86%E5%B8%83"><span class="toc-number">2.3.</span> <span class="toc-text">3. L1和L2正则分别服从什么分布</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%AF%B9%E4%BA%8E%E6%A0%91%E5%BD%A2%E7%BB%93%E6%9E%84%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E7%94%A8%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%9F"><span class="toc-number">2.4.</span> <span class="toc-text">4. 对于树形结构为什么不用归一化？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%B9%B3%E8%A1%A1%EF%BC%9F%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%EF%BC%9F"><span class="toc-number">2.5.</span> <span class="toc-text">5. 什么是数据不平衡？如何解决？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-bias%EF%BC%8Cvariance%EF%BC%8Cerror"><span class="toc-number">2.6.</span> <span class="toc-text">6. bias，variance，error</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">3.</span> <span class="toc-text">三. 线性回归与逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-logistic%E5%9B%9E%E5%BD%92%E5%85%AC%E5%BC%8F%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">3.1.</span> <span class="toc-text">1. logistic回归公式是什么？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%92%8C%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%EF%BC%8C%E6%9C%89%E4%BD%95%E5%BC%82%E5%90%8C%EF%BC%9F"><span class="toc-number">3.2.</span> <span class="toc-text">2. 逻辑回归和线性回归，有何异同？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">3.3.</span> <span class="toc-text">3. 逻辑回归的损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%A4%84%E7%90%86%E5%A4%9A%E5%88%86%E7%B1%BB%E6%A0%87%E7%AD%BE%E9%97%AE%E9%A2%98%E6%97%B6%EF%BC%8C%E4%B8%80%E8%88%AC%E6%80%8E%E4%B9%88%E5%81%9A%EF%BC%9F"><span class="toc-number">3.4.</span> <span class="toc-text">4. 逻辑回归处理多分类标签问题时，一般怎么做？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.</span> <span class="toc-text">四. 朴素贝叶斯模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%85%A8%E6%A6%82%E7%8E%87%E5%85%AC%E5%BC%8F-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F"><span class="toc-number">4.1.</span> <span class="toc-text">1. 全概率公式 &amp; 贝叶斯公式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AB%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%9F"><span class="toc-number">4.2.</span> <span class="toc-text">2. 为什么叫朴素贝叶斯？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%EF%BC%9F"><span class="toc-number">4.3.</span> <span class="toc-text">3. 工作流程？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">4.4.</span> <span class="toc-text">4. 优缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AD-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0"><span class="toc-number">5.</span> <span class="toc-text">六. 集成学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%EF%BC%9F"><span class="toc-number">5.1.</span> <span class="toc-text">1. 什么是集成学习算法？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B8%BB%E8%A6%81%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A1%86%E6%9E%B6%EF%BC%9F"><span class="toc-number">5.2.</span> <span class="toc-text">2. 集成学习主要有哪些框架？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%B8%B8%E7%94%A8%E7%9A%84bagging%EF%BC%88Boostrap-Agregation%EF%BC%89%E7%AE%97%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">5.3.</span> <span class="toc-text">3. 常用的bagging（Boostrap Agregation）算法有哪些？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%B8%B8%E7%94%A8%E7%9A%84Boosting%E7%AE%97%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">5.4.</span> <span class="toc-text">4. 常用的Boosting算法有哪些？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E5%B8%B8%E7%94%A8%E7%9A%84stacking%E7%AE%97%E6%B3%95%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">5.5.</span> <span class="toc-text">5. 常用的stacking算法有哪些？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%83-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">6.</span> <span class="toc-text">七.  随机森林</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%AE%80%E8%BF%B0%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><span class="toc-number">6.1.</span> <span class="toc-text">1. 简述随机森林算法原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%9A%84%E9%9A%8F%E6%9C%BA%E6%80%A7%E4%BD%93%E7%8E%B0%E5%9C%A8%E5%93%AA%EF%BC%9F"><span class="toc-number">6.2.</span> <span class="toc-text">2. 随机森林的随机性体现在哪？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">6.3.</span> <span class="toc-text">3. 随机森林算法的优缺点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E8%83%BD%E7%94%A8%E5%85%A8%E6%A0%B7%E6%9C%AC%E5%8E%BB%E8%AE%AD%E7%BB%83m%E6%A3%B5%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%9F"><span class="toc-number">6.4.</span> <span class="toc-text">4. 随机森林为什么不能用全样本去训练m棵决策树？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%92%8CGBDT%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">6.5.</span> <span class="toc-text">5. 随机森林和GBDT的区别</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AB-Adaboost%EF%BC%88%E8%87%AA%E9%80%82%E5%BA%94%E5%A2%9E%E5%BC%BA%EF%BC%89"><span class="toc-number">7.</span> <span class="toc-text">八. Adaboost（自适应增强）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%AE%80%E8%BF%B0Adaboost%E5%8E%9F%E7%90%86"><span class="toc-number">7.1.</span> <span class="toc-text">1. 简述Adaboost原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Adaboost%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">7.2.</span> <span class="toc-text">2. Adaboost常用的损失函数有哪些？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Adaboost%E5%A6%82%E4%BD%95%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%EF%BC%9F"><span class="toc-number">7.3.</span> <span class="toc-text">3. Adaboost如何用于分类？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-Adaboost%E7%AE%97%E6%B3%95%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%EF%BC%9F"><span class="toc-number">7.4.</span> <span class="toc-text">4. Adaboost算法的优缺点？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B9%9D-GBDT"><span class="toc-number">8.</span> <span class="toc-text">九. GBDT</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%AE%80%E8%BF%B0GBDT%E5%8E%9F%E7%90%86"><span class="toc-number">8.1.</span> <span class="toc-text">1. 简述GBDT原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-GBDT%E5%B8%B8%E7%94%A8%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">8.2.</span> <span class="toc-text">2. GBDT常用的损失函数有哪些？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-GBDT%E5%A6%82%E4%BD%95%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB"><span class="toc-number">8.3.</span> <span class="toc-text">3. GBDT如何用于分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-GBDT%E7%AE%97%E6%B3%95%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%EF%BC%9F"><span class="toc-number">8.4.</span> <span class="toc-text">4. GBDT算法的优缺点？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%81-XGBoost"><span class="toc-number">9.</span> <span class="toc-text">十.  XGBoost</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%AE%80%E8%BF%B0XGBoost"><span class="toc-number">9.1.</span> <span class="toc-text">1. 简述XGBoost</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-XGBoost%E5%92%8CGBDT%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%8D%E5%90%8C%EF%BC%9F"><span class="toc-number">9.2.</span> <span class="toc-text">2. XGBoost和GBDT有什么不同？</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%882%EF%BC%89/" title="机器学习笔记（2）"><img src="/cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习笔记（2）"/></a><div class="content"><a class="title" href="/2024/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%882%EF%BC%89/" title="机器学习笔记（2）">机器学习笔记（2）</a><time datetime="2024-10-10T15:10:40.000Z" title="发表于 2024-10-10 23:10:40">2024-10-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89/" title="机器学习笔记（1）"><img src="/cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="机器学习笔记（1）"/></a><div class="content"><a class="title" href="/2024/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89/" title="机器学习笔记（1）">机器学习笔记（1）</a><time datetime="2024-10-10T15:10:23.000Z" title="发表于 2024-10-10 23:10:23">2024-10-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/10/10/python%E8%80%83%E7%82%B9/" title="python考点"><img src="/cover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="python考点"/></a><div class="content"><a class="title" href="/2024/10/10/python%E8%80%83%E7%82%B9/" title="python考点">python考点</a><time datetime="2024-10-10T15:02:56.000Z" title="发表于 2024-10-10 23:02:56">2024-10-10</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/cover.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2024 By Simon Ding</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">希望自己的日积月累，终能成他人的望尘莫及。</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>